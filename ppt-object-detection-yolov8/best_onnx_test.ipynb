{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "from pathlib import Path\n",
    "import time\n",
    "from queue import Queue\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thiru\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\nn\\tasks.py:714: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.29 ðŸš€ Python-3.10.10 torch-2.5.1+cu118 CPU (12th Gen Intel Core(TM) i9-12900HK)\n",
      "Model summary (fused): 168 layers, 11129454 parameters, 0 gradients, 28.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'C:\\Learning\\AIPC\\YoloV8\\data\\runs\\detect\\yolov8s_ppe_css_50_epochs\\weights\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 14, 8400) (21.5 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 0.7s, saved as 'C:\\Learning\\AIPC\\YoloV8\\data\\runs\\detect\\yolov8s_ppe_css_50_epochs\\weights\\best.onnx' (42.5 MB)\n",
      "\n",
      "Export complete (2.5s)\n",
      "Results saved to \u001b[1mC:\\Learning\\AIPC\\YoloV8\\data\\runs\\detect\\yolov8s_ppe_css_50_epochs\\weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=C:\\Learning\\AIPC\\YoloV8\\data\\runs\\detect\\yolov8s_ppe_css_50_epochs\\weights\\best.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=C:\\Learning\\AIPC\\YoloV8\\data\\runs\\detect\\yolov8s_ppe_css_50_epochs\\weights\\best.onnx imgsz=640 data=./data.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Learning\\\\AIPC\\\\YoloV8\\\\data\\\\runs\\\\detect\\\\yolov8s_ppe_css_50_epochs\\\\weights\\\\best.onnx'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the PyTorch model\n",
    "model = YOLO(r\"C:\\Learning\\AIPC\\YoloV8\\data\\runs\\detect\\yolov8s_ppe_css_50_epochs\\weights\\best.pt\")  # Provide path to your best.pt model\n",
    "\n",
    "# Export to ONNX format\n",
    "model.export(format=\"onnx\", imgsz=640, dynamic=True)  # 640 is the input size, change if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed image saved to: C:\\Learning\\AIPC\\YoloV8\\data\\css-data\\test\\images\\output.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Export model to ONNX format\n",
    "def export_model_to_onnx(model_path, onnx_path, input_size=640):\n",
    "    model = YOLO(r\"C:\\Learning\\AIPC\\YoloV8\\data\\runs\\detect\\yolov8s_ppe_css_50_epochs\\weights\\best.pt\")  # Load the PyTorch model\n",
    "    model.export(format=\"onnx\", imgsz=input_size, dynamic=True)  # Export to ONNX format\n",
    "\n",
    "# Class to handle YOLOv8 model inference using ONNX\n",
    "class YOLOv8ONNX:\n",
    "    def __init__(self, model_path, input_size=640):\n",
    "        # Load the ONNX model using ONNX Runtime\n",
    "        self.session = ort.InferenceSession(model_path, providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "        self.input_name = self.session.get_inputs()[0].name\n",
    "        self.output_name = self.session.get_outputs()[0].name\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def preprocess(self, frame):\n",
    "        \"\"\"Preprocess a single frame/image.\"\"\"\n",
    "        original_shape = frame.shape[:2]\n",
    "        resized = cv2.resize(frame, (self.input_size, self.input_size))\n",
    "        rgb_image = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n",
    "        normalized = rgb_image / 255.0\n",
    "        transposed = np.transpose(normalized, (2, 0, 1))  # HWC -> CHW format\n",
    "        input_tensor = np.expand_dims(transposed, axis=0).astype(np.float32)\n",
    "        return input_tensor, original_shape\n",
    "\n",
    "    def run_inference(self, batch_tensor):\n",
    "        \"\"\"Run inference on the image.\"\"\"\n",
    "        outputs = self.session.run([self.output_name], {self.input_name: batch_tensor})\n",
    "        return outputs\n",
    "\n",
    "    def postprocess(self, outputs, original_shapes, conf_threshold=0.25):\n",
    "        \"\"\"Postprocess model outputs to extract bounding boxes.\"\"\"\n",
    "        all_boxes = []\n",
    "        for idx, predictions in enumerate(outputs[0]):\n",
    "            boxes = []\n",
    "            for pred in predictions:\n",
    "                conf = pred[4] * pred[5]\n",
    "                if conf > conf_threshold:\n",
    "                    x, y, w, h = pred[:4]\n",
    "                    x1 = int((x - w / 2) * original_shapes[idx][1])\n",
    "                    y1 = int((y - h / 2) * original_shapes[idx][0])\n",
    "                    x2 = int((x + w / 2) * original_shapes[idx][1])\n",
    "                    y2 = int((y + h / 2) * original_shapes[idx][0])\n",
    "                    boxes.append((x1, y1, x2, y2, conf))\n",
    "            all_boxes.append(boxes)\n",
    "        return all_boxes\n",
    "\n",
    "# Image processing function\n",
    "def process_image(input_image_path, output_image_path, model_path, input_size=640, conf_threshold=0.25):\n",
    "    # Initialize the ONNX model\n",
    "    model = YOLOv8ONNX(model_path, input_size)\n",
    "\n",
    "    # Read the image\n",
    "    image = cv2.imread(input_image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Failed to load image from path: {input_image_path}\")\n",
    "\n",
    "    # Preprocess the image\n",
    "    input_tensor, original_shape = model.preprocess(image)\n",
    "\n",
    "    # Run inference\n",
    "    outputs = model.run_inference(input_tensor)\n",
    "\n",
    "    # Postprocess the results\n",
    "    detections = model.postprocess(outputs, [original_shape], conf_threshold)\n",
    "\n",
    "    # Draw bounding boxes on the image\n",
    "    for x1, y1, x2, y2, conf in detections[0]:  # Only one image (batch size 1)\n",
    "        x1, y1, x2, y2 = max(0, x1), max(0, y1), min(x2, image.shape[1]), min(y2, image.shape[0])\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"{conf:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Ensure the output path has the correct file name (add extension if missing)\n",
    "    if not output_image_path.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        output_image_path += '.jpg'\n",
    "\n",
    "    # Save the output image and check if it was successful\n",
    "    if cv2.imwrite(output_image_path, image):\n",
    "        print(f\"Processed image saved to: {output_image_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to save processed image to: {output_image_path}\")\n",
    "\n",
    "# Video processing function\n",
    "def process_video(input_video_path, output_video_path, model_path, input_size=640, conf_threshold=0.25, batch_size=4):\n",
    "    # Initialize the ONNX model\n",
    "    model = YOLOv8ONNX(model_path, input_size)\n",
    "\n",
    "    # Open video capture\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Failed to open video file: {input_video_path}\")\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_size = (frame_width, frame_height)\n",
    "\n",
    "    # Test with MJPG codec (more universal)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')  # 'MJPG' codec should work on most systems\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, frame_size)\n",
    "\n",
    "    # Check if the VideoWriter was opened successfully\n",
    "    if not out.isOpened():\n",
    "        raise ValueError(f\"Failed to open video writer for output file: {output_video_path}\")\n",
    "\n",
    "    frame_count = 0  # Track the number of frames processed\n",
    "    frame_queue = []\n",
    "    results_queue = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"End of video reached. Processed {frame_count} frames.\")\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        # Preprocess frame\n",
    "        input_tensor, original_shape = model.preprocess(frame)\n",
    "\n",
    "        # Run inference on a batch (in this case, we are processing one frame at a time)\n",
    "        frame_queue.append(input_tensor)\n",
    "        if len(frame_queue) == batch_size:\n",
    "            batch_tensor = np.vstack(frame_queue)\n",
    "            outputs = model.run_inference(batch_tensor)\n",
    "            detections = model.postprocess(outputs, [original_shape] * batch_size, conf_threshold)\n",
    "            for idx, det in enumerate(detections):\n",
    "                results_queue.append((frame_queue[idx], det))\n",
    "            frame_queue = []\n",
    "\n",
    "        # Process results and write frames\n",
    "        if results_queue:\n",
    "            frame, detections = results_queue.pop(0)\n",
    "            frame = np.squeeze(frame, axis=0)  # Remove batch dimension\n",
    "            frame = np.transpose(frame, (1, 2, 0))  # (C, H, W) -> (H, W, C)\n",
    "            frame = (frame * 255).astype(np.uint8)  # Denormalize\n",
    "\n",
    "            # Convert from RGB to BGR before writing to video\n",
    "            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Draw bounding boxes on the frame\n",
    "            for x1, y1, x2, y2, conf in detections:\n",
    "                x1, y1, x2, y2 = max(0, x1), max(0, y1), min(x2, frame.shape[1]), min(y2, frame.shape[0])\n",
    "                cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame_bgr, f\"{conf:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            # Write the frame to the output video\n",
    "            out.write(frame_bgr)\n",
    "        \n",
    "    # Release video resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Video processing completed. {frame_count} frames processed.\")\n",
    "\n",
    "# Process input based on file type (image or video)\n",
    "def process_input(input_path, output_path, model_path, input_size=640, conf_threshold=0.25, batch_size=4):\n",
    "    # Check if the input is a video or an image\n",
    "    file_extension = os.path.splitext(input_path)[-1].lower()\n",
    "    \n",
    "    if file_extension in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']:\n",
    "        # Process as an image\n",
    "        process_image(input_path, output_path, model_path, input_size, conf_threshold)\n",
    "    elif file_extension in ['.mp4', '.avi', '.mov', '.mkv', '.flv']:\n",
    "        # Process as a video\n",
    "        process_video(input_path, output_path, model_path, input_size, conf_threshold, batch_size)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please upload an image or video.\")\n",
    "\n",
    "# Example usage\n",
    "input_path = r\"C:\\Learning\\AIPC\\YoloV8\\data\\css-data\\test\\images\\777_jpg.rf.92dc6945342410ced7ac93f3dfbff0c5.jpg\"  # Replace with your input file path (image or video)\n",
    "output_path = r\"C:\\Learning\\AIPC\\YoloV8\\data\\css-data\\test\\images\\output.jpg\"  # Output file path\n",
    "onnx_model = r\"C:\\Learning\\AIPC\\YoloV8\\data\\runs\\detect\\yolov8s_ppe_css_50_epochs\\weights\\best.onnx\"  # Path to the ONNX model\n",
    "\n",
    "process_input(input_path, output_path, onnx_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 29.0, Width: 1280.0, Height: 720.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "video_path = r\"C:\\Learning\\AIPC\\YoloV8\\redZone\\css-data\\example_video.mp4\" \n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if cap.isOpened():\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    print(f\"FPS: {fps}, Width: {width}, Height: {height}\")\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
